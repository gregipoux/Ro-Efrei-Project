# Alors l√†, ce fichier contient toutes les fonctions pour l'√©tude de la complexit√© des algorithmes
# En r√©sum√©, on g√©n√®re des probl√®mes al√©atoires, on mesure les temps d'ex√©cution, et on trace des graphiques
# Pour faire simple : on veut savoir combien de temps prennent nos algorithmes selon la taille du probl√®me

import random
import time
import os
import sys
import gc  # Garbage collection pour lib√©rer la m√©moire entre les it√©rations
from typing import List, Tuple, Dict
import json
from multiprocessing import Pool, cpu_count
from functools import partial
import traceback

# Import matplotlib for plotting
try:
    import matplotlib.pyplot as plt
    MATPLOTLIB_AVAILABLE = True
except ImportError:
    MATPLOTLIB_AVAILABLE = False

from transport_problem import (
    northwest_corner_method,
    balas_hammer_method,
    compute_total_cost
)
from marche_pied import rendre_connexe, trouver_cycle_avec_arete
from cyclique import tester_acyclique, maximiser_sur_cycle
from connexite import is_connected_transport
from potentiels import (
    calculer_potentiels,
    calculer_couts_marginaux,
    detecter_arete_ameliorante
)


def calculer_nb_processus_optimal(nb_processus_desire: int = None) -> int:
    """
    Calcule le nombre optimal de processus √† utiliser pour √©viter la surchauffe.
    
    Args:
        nb_processus_desire: Nombre de processus souhait√© (None pour auto)
    
    Returns:
        Nombre de processus recommand√©
    """
    nb_cores = cpu_count()
    
    if nb_processus_desire is not None and nb_processus_desire > 0:
        # Respecter le choix de l'utilisateur mais limiter au maximum disponible
        return min(nb_processus_desire, nb_cores)
    
    # Par d√©faut : laisser au moins 1-2 c≈ìurs libres
    if nb_cores <= 2:
        return 1
    elif nb_cores <= 4:
        return nb_cores - 1
    else:
        # Pour les syst√®mes avec beaucoup de c≈ìurs, laisser 2 c≈ìurs libres
        return max(1, nb_cores - 2)


def generer_probleme_aleatoire(n: int, seed: int = None) -> Tuple[List[List[float]], List[float], List[float]]:
    # Alors l√†, cette fonction g√©n√®re un probl√®me de transport al√©atoire de taille n √ó n
    # En r√©sum√©, on g√©n√®re les co√ªts ai,j entre 1 et 100, puis on g√©n√®re une matrice temp pour calculer les provisions et commandes
    # Pour faire simple : on veut un probl√®me √©quilibr√© (somme provisions = somme commandes)
    # Optimis√© : on utilise numpy si disponible pour acc√©l√©rer les calculs
    
    # Pseudo-code :
    # SI seed est fourni:
    #     Initialiser le g√©n√©rateur al√©atoire avec seed (pour reproductibilit√©)
    # FIN SI
    # 
    # G√©n√©rer matrice des co√ªts : pour chaque (i,j), ai,j = nombre al√©atoire entre 1 et 100
    # G√©n√©rer matrice temp : pour chaque (i,j), tempi,j = nombre al√©atoire entre 1 et 100
    # 
    # Calculer provisions : Pi = somme des tempi,j sur j (pour chaque ligne)
    # Calculer commandes : Cj = somme des tempi,j sur i (pour chaque colonne)
    # 
    # RETOURNER (costs, supplies, demands)
    
    # Essayer d'utiliser numpy pour acc√©l√©rer (mais on garde la compatibilit√© sans numpy)
    
    if seed is not None:
        random.seed(seed)
    
    try:
        import numpy as np
        
        # G√©n√©rer les co√ªts et la matrice temporaire en une fois avec numpy
        costs = np.random.randint(1, 101, size=(n, n)).astype(float).tolist()
        temp_matrix = np.random.randint(1, 101, size=(n, n)).astype(float)
        
        # Calculer les sommes (provisions et commandes)
        supplies = np.sum(temp_matrix, axis=1).tolist()
        demands = np.sum(temp_matrix, axis=0).tolist()
        
        return costs, supplies, demands
        
    except ImportError:
        # Fallback si numpy n'est pas install√©
        costs = [[float(random.randint(1, 100)) for _ in range(n)] for _ in range(n)]
        
        # Pour garantir un probl√®me √©quilibr√©, on g√©n√®re une matrice al√©atoire temporaire
        # et on calcule les sommes lignes/colonnes
        temp_matrix = [[float(random.randint(1, 100)) for _ in range(n)] for _ in range(n)]
        
        supplies = [sum(row) for row in temp_matrix]
        demands = [sum(temp_matrix[i][j] for i in range(n)) for j in range(n)]
    
    return costs, supplies, demands


def resoudre_marche_pied_silencieux(
    costs: List[List[float]],
    supplies: List[float],
    demands: List[float],
    allocation_initiale: List[List[float]],
    max_duration: float = 30.0
) -> Tuple[List[List[float]], int]:
    # Alors l√†, cette fonction r√©sout le probl√®me avec la m√©thode du marche-pied, mais sans afficher quoi que ce soit
    # En r√©sum√©, c'est la m√™me chose que dans main.py mais sans les print, pour pouvoir mesurer les temps proprement
    # Pour faire simple : on optimise jusqu'√† trouver la solution optimale, et on compte les it√©rations
    
    # Pseudo-code :
    # allocation = copie(allocation_initiale)
    # nb_iterations = 0
    # 
    # TANT QUE nb_iterations < max_iterations:
    #     √âtape 1 : Rendre acyclique (casser tous les cycles de mani√®re r√©p√©t√©e)
    #     √âtape 2 : Rendre connexe (ajouter des ar√™tes si besoin)
    #     √âtape 3 : Calculer les potentiels
    #     √âtape 4 : Calculer les co√ªts marginaux
    #     √âtape 5 : D√©tecter l'ar√™te am√©liorante
    #     SI pas d'ar√™te am√©liorante:
    #         RETOURNER solution optimale (on a fini !)
    #     FIN SI
    #     √âtape 6 : Ajouter l'ar√™te et trouver le cycle
    #     √âtape 7 : Maximiser sur le cycle
    # FIN TANT QUE
    
    allocation = [row.copy() for row in allocation_initiale]
    nb_iterations = 0
    # OPTIMISATION : R√©duire le nombre max d'it√©rations pour les tr√®s grandes tailles
    n = len(costs)
    if n >= 10000:
        max_iterations = 50  # Limiter drastiquement pour n=10000
    elif n >= 5000:
        max_iterations = 100
    elif n >= 1000:
        max_iterations = 200
    else:
        max_iterations = 1000  # Protection contre les boucles infinies
    max_cycles_elimination = 50 if n >= 1000 else 100  # R√©duire pour les grandes tailles
    debut_boucle = time.perf_counter()
    debut_global = debut_boucle
    # Pour les grandes valeurs de n, faire un garbage collection plus fr√©quent
    gc_frequency = 1 if n >= 1000 else 10  # GC √† chaque it√©ration pour n>=1000, sinon toutes les 10 it√©rations
    
    while nb_iterations < max_iterations:
        nb_iterations += 1
        
        # Protection globale : on arr√™te si on d√©passe la dur√©e maximale autoris√©e
        if time.perf_counter() - debut_global > max_duration:
            break
        
        # Protection contre les boucles trop longues (plus de 30 secondes pour une it√©ration)
        if nb_iterations > 1:
            temps_boucle = time.perf_counter() - debut_boucle
            if temps_boucle > max_duration:
                break
        
        # Garbage collection p√©riodique pour les grandes valeurs de n
        if n >= 1000 and nb_iterations % gc_frequency == 0:
            gc.collect()
        
        # √âtape 1 : D√©tecter et √©liminer les cycles de mani√®re r√©p√©t√©e
        cycles_elimines = 0
        while cycles_elimines < max_cycles_elimination:
            if time.perf_counter() - debut_global > max_duration:
                break
            try:
                result_acyclique = tester_acyclique(allocation)
                if not isinstance(result_acyclique, tuple) or len(result_acyclique) != 2:
                    raise ValueError(f"tester_acyclique a retourn√© un r√©sultat inattendu: {type(result_acyclique)}, attendu: Tuple[bool, List[Tuple[int, int]]]")
                acyclique, cycle = result_acyclique
            except ValueError as e:
                print(f"  ‚ö† Erreur dans tester_acyclique: {e}", file=sys.stderr, flush=True)
                raise
            if acyclique:
                break
            
            cycles_elimines += 1
            delta = maximiser_sur_cycle(allocation, cycle, verbose=False)
            
            if delta <= 1e-9:
                # Cas particulier : delta = 0, on casse le cycle structurellement
                if cycle:
                    i, j = cycle[0]
                    allocation[i][j] = 0.0
                # Forcer la sortie apr√®s avoir cass√© le cycle
                break
        
        if cycles_elimines >= max_cycles_elimination:
            # Trop de cycles, on arr√™te pour √©viter une boucle infinie
            break
        
        # √âtape 2 : V√©rifier la connexit√©
        try:
            result_connexite = is_connected_transport(allocation)
            if not isinstance(result_connexite, tuple) or len(result_connexite) != 2:
                raise ValueError(f"is_connected_transport a retourn√© un r√©sultat inattendu: {type(result_connexite)}, attendu: Tuple[bool, List]")
            est_connexe, _ = result_connexite
        except ValueError as e:
            print(f"  ‚ö† Erreur dans is_connected_transport: {e}", file=sys.stderr, flush=True)
            raise
        ar√™tes_ajout√©es_connexit√© = []
        
        if not est_connexe:
            # Rendre connexe en ajoutant des ar√™tes de co√ªt minimal
            ar√™tes_ajout√©es_connexit√© = rendre_connexe(costs, allocation, supplies, demands, verbose=False)
            
            # V√©rifier √† nouveau les cycles apr√®s connexit√©
            cycles_elimines_apres = 0
            while cycles_elimines_apres < max_cycles_elimination:
                if time.perf_counter() - debut_global > max_duration:
                    break
                try:
                    result_acyclique = tester_acyclique(allocation)
                    if not isinstance(result_acyclique, tuple) or len(result_acyclique) != 2:
                        raise ValueError(f"tester_acyclique a retourn√© un r√©sultat inattendu: {type(result_acyclique)}, attendu: Tuple[bool, List[Tuple[int, int]]]")
                    acyclique, cycle = result_acyclique
                except ValueError as e:
                    print(f"  ‚ö† Erreur dans tester_acyclique (apr√®s connexit√©): {e}", file=sys.stderr, flush=True)
                    raise
                if acyclique:
                    break
                
                cycles_elimines_apres += 1
                delta = maximiser_sur_cycle(allocation, cycle, verbose=False)
                
                if delta <= 1e-9:
                    if cycle:
                        i, j = cycle[0]
                        allocation[i][j] = 0.0
                    # Forcer la sortie apr√®s avoir cass√© le cycle
                    break
            
            if cycles_elimines_apres >= max_cycles_elimination:
                # Trop de cycles, on arr√™te pour √©viter une boucle infinie
                break
        
        # √âtape 3 : Calculer les potentiels
        try:
            result_potentiels = calculer_potentiels(costs, allocation)
            if not isinstance(result_potentiels, tuple) or len(result_potentiels) != 2:
                raise ValueError(f"calculer_potentiels a retourn√© un r√©sultat inattendu: {type(result_potentiels)}, attendu: Tuple[List[float], List[float]]")
            u, v = result_potentiels
        except ValueError as e:
            print(f"  ‚ö† Erreur dans calculer_potentiels: {e}", file=sys.stderr, flush=True)
            raise
        
        # √âtape 4 & 5 : D√©tecter l'ar√™te am√©liorante (Optimis√©)
        # Utiliser la strat√©gie "first" pour les grands probl√®mes (toujours pour n >= 1000)
        strategy = "first" if len(costs) >= 1000 else ("first" if len(costs) >= 500 else "best")
        
        # On utilise la version rapide qui n'alloue pas la matrice des marginaux
        from potentiels import detecter_arete_ameliorante_rapide
        arete_ameliorante = detecter_arete_ameliorante_rapide(costs, u, v, allocation, strategy=strategy)
        
        if arete_ameliorante is None:
            # Solution optimale trouv√©e !
            break
        
        # V√©rifier que l'ar√™te am√©liorante est bien un tuple de 3 √©l√©ments
        if not isinstance(arete_ameliorante, tuple) or len(arete_ameliorante) != 3:
            raise ValueError(f"detecter_arete_ameliorante_rapide a retourn√© un r√©sultat inattendu: {type(arete_ameliorante)}, attendu: Tuple[int, int, float] ou None")
        
        i_ameliorant, j_ameliorant, _ = arete_ameliorante
        
        # √âtape 6 : Ajouter l'ar√™te am√©liorante et trouver le cycle
        allocation[i_ameliorant][j_ameliorant] = 1.0
        cycle = trouver_cycle_avec_arete(allocation, i_ameliorant, j_ameliorant)
        
        # √âtape 7 : Maximiser le transport sur le cycle
        delta = maximiser_sur_cycle(allocation, cycle, verbose=False)
        
        if delta <= 1e-9:
            # Cas particulier : delta = 0
            for i, j in ar√™tes_ajout√©es_connexit√©:
                allocation[i][j] = 0.0
            allocation[i_ameliorant][j_ameliorant] = 1e-6
    
    # Garbage collection final avant de retourner (important pour N=10000)
    gc.collect()
    
    return allocation, nb_iterations


def mesurer_temps_nord_ouest(costs: List[List[float]], supplies: List[float], demands: List[float]) -> float:
    # Alors l√†, cette fonction mesure le temps d'ex√©cution de l'algorithme Nord-Ouest
    # En clair, on prend le temps avant, on ex√©cute l'algo, on prend le temps apr√®s, et on fait la diff√©rence
    
    # Garbage collection avant mesure pour lib√©rer la m√©moire (important pour N=10000)
    gc.collect()
    
    start_time = time.perf_counter()
    result = northwest_corner_method(supplies, demands)
    end_time = time.perf_counter()
    
    # V√©rifier que northwest_corner_method retourne bien une allocation
    if not isinstance(result, list):
        raise ValueError(f"northwest_corner_method a retourn√© un type inattendu: {type(result)}, attendu: List[List[float]]")
    
    # Garbage collection apr√®s mesure
    gc.collect()
    
    return end_time - start_time


def mesurer_temps_balas_hammer(costs: List[List[float]], supplies: List[float], demands: List[float]) -> float:
    # Alors l√†, cette fonction mesure le temps d'ex√©cution de l'algorithme Balas-Hammer
    # M√™me principe que pour Nord-Ouest : on mesure le temps d'ex√©cution
    
    # Garbage collection avant mesure pour lib√©rer la m√©moire
    gc.collect()
    
    # D√©terminer une dur√©e max adapt√©e √† la taille
    n = len(costs)
    # Timeout plus long pour les tr√®s grandes valeurs (N=10000 peut prendre plusieurs minutes)
    max_duration = 300.0 if n >= 5000 else 60.0 if n >= 1000 else 10.0 if n >= 500 else 20.0 if n >= 200 else 30.0
    
    start_time = time.perf_counter()
    result = balas_hammer_method(costs, supplies, demands, verbose=False, max_duration=max_duration)
    end_time = time.perf_counter()
    
    # V√©rifier que balas_hammer_method retourne bien une allocation
    if not isinstance(result, list):
        raise ValueError(f"balas_hammer_method a retourn√© un type inattendu: {type(result)}, attendu: List[List[float]]")
    
    # Garbage collection apr√®s mesure
    gc.collect()
    
    return end_time - start_time


def mesurer_temps_marche_pied_no(
    costs: List[List[float]],
    supplies: List[float],
    demands: List[float]
) -> float:
    # Alors l√†, cette fonction mesure le temps d'ex√©cution de la m√©thode du marche-pied avec solution initiale Nord-Ouest
    # En r√©sum√©, on calcule d'abord la solution initiale avec NO, puis on optimise avec le marche-pied
    
    try:
        # Calculer la solution initiale avec Nord-Ouest
        allocation_initiale = northwest_corner_method(supplies, demands)
        
        # D√©terminer une dur√©e max adapt√©e √† la taille
        n = len(costs)
        # Timeout plus long pour les tr√®s grandes valeurs (N=10000 peut prendre plusieurs minutes)
        max_duration = 300.0 if n >= 5000 else 60.0 if n >= 1000 else 10.0 if n >= 500 else 20.0 if n >= 200 else 30.0
        
        # Garbage collection avant mesure pour lib√©rer la m√©moire
        gc.collect()
        
        # Mesurer le temps du marche-pied avec garde de dur√©e
        start_time = time.perf_counter()
        result = resoudre_marche_pied_silencieux(costs, supplies, demands, allocation_initiale, max_duration=max_duration)
        end_time = time.perf_counter()
        
        # V√©rifier que le r√©sultat est correct (allocation, nb_iterations)
        if not isinstance(result, tuple) or len(result) != 2:
            raise ValueError(f"resoudre_marche_pied_silencieux a retourn√© un r√©sultat inattendu: {type(result)}, attendu: Tuple[List[List[float]], int]")
        
        # Garbage collection apr√®s mesure
        gc.collect()
        
        return end_time - start_time
    except Exception as e:
        # En cas d'erreur, afficher plus de d√©tails pour le d√©bogage
        import traceback
        print(f"  ‚ö† Erreur d√©taill√©e dans mesurer_temps_marche_pied_no: {e}", file=sys.stderr, flush=True)
        traceback.print_exc(file=sys.stderr)
        raise


def mesurer_temps_marche_pied_bh(
    costs: List[List[float]],
    supplies: List[float],
    demands: List[float]
) -> float:
    # Alors l√†, cette fonction mesure le temps d'ex√©cution de la m√©thode du marche-pied avec solution initiale Balas-Hammer
    # M√™me principe que pour NO, mais avec Balas-Hammer comme solution initiale
    
    try:
        # D√©terminer une dur√©e max adapt√©e √† la taille
        n = len(costs)
        # Timeout plus long pour les tr√®s grandes valeurs (N=10000 peut prendre plusieurs minutes)
        max_duration = 300.0 if n >= 5000 else 60.0 if n >= 1000 else 10.0 if n >= 500 else 20.0 if n >= 200 else 30.0
        
        # Garbage collection avant mesure pour lib√©rer la m√©moire
        gc.collect()
        
        # Calculer la solution initiale avec Balas-Hammer
        allocation_initiale = balas_hammer_method(costs, supplies, demands, verbose=False, max_duration=max_duration)
        
        # V√©rifier que balas_hammer_method retourne bien une allocation (liste de listes)
        if not isinstance(allocation_initiale, list):
            raise ValueError(f"balas_hammer_method a retourn√© un type inattendu: {type(allocation_initiale)}, attendu: List[List[float]]")
        
        # Garbage collection apr√®s Balas-Hammer
        gc.collect()
        
        # Mesurer le temps du marche-pied avec garde de dur√©e
        start_time = time.perf_counter()
        result = resoudre_marche_pied_silencieux(costs, supplies, demands, allocation_initiale, max_duration=max_duration)
        end_time = time.perf_counter()
        
        # V√©rifier que le r√©sultat est correct (allocation, nb_iterations)
        if not isinstance(result, tuple) or len(result) != 2:
            raise ValueError(f"resoudre_marche_pied_silencieux a retourn√© un r√©sultat inattendu: {type(result)}, attendu: Tuple[List[List[float]], int]")
        
        # Garbage collection apr√®s mesure
        gc.collect()
        
        return end_time - start_time
    except Exception as e:
        # En cas d'erreur, afficher plus de d√©tails pour le d√©bogage
        import traceback
        print(f"  ‚ö† Erreur d√©taill√©e dans mesurer_temps_marche_pied_bh: {e}", file=sys.stderr, flush=True)
        traceback.print_exc(file=sys.stderr)
        raise


def executer_une_iteration_complete(n: int, seed: int) -> Tuple[float, float, float, float]:
    # Alors l√†, cette fonction ex√©cute une it√©ration compl√®te : g√©n√®re un probl√®me et mesure tous les temps
    # En r√©sum√©, c'est une fonction helper pour la parall√©lisation
    # Pour faire simple : on fait tout en une fois pour pouvoir parall√©liser facilement
    
    import os
    pid = os.getpid()
    
    try:
        # G√©n√©rer un probl√®me al√©atoire
        costs, supplies, demands = generer_probleme_aleatoire(n, seed=seed)

        # OPTIMISATION : Pour les grandes tailles (n >= 1000), √©viter de cloner plusieurs fois
        # Chaque mesure modifie potentiellement les listes en place.
        # On clone donc les donn√©es de base pour isoler chaque mesure.
        # Pour n >= 1000, on clone seulement quand n√©cessaire pour √©conomiser la m√©moire
        def clones():
            return (
                [row.copy() for row in costs],
                supplies.copy(),
                demands.copy(),
            )
        
        # OPTIMISATION : Pour n >= 1000, r√©utiliser les clones quand possible
        # Mesurer tous les temps avec gestion d'erreur individuelle
        try:
            c, s, d = clones()
            temps_no = mesurer_temps_nord_ouest(c, s, d)
            # Lib√©rer la m√©moire imm√©diatement apr√®s utilisation
            del c, s, d
            gc.collect()
        except Exception as e:
            print(f"[PID {pid}] ‚ö† Erreur dans mesurer_temps_nord_ouest (n={n}, seed={seed}): {e}", file=sys.stderr, flush=True)
            temps_no = 0.0
        
        try:
            c, s, d = clones()
            temps_bh = mesurer_temps_balas_hammer(c, s, d)
            del c, s, d
            gc.collect()
        except Exception as e:
            print(f"[PID {pid}] ‚ö† Erreur dans mesurer_temps_balas_hammer (n={n}, seed={seed}): {e}", file=sys.stderr, flush=True)
            temps_bh = 0.0
        
        try:
            c, s, d = clones()
            temps_marche_pied_no = mesurer_temps_marche_pied_no(c, s, d)
            del c, s, d
            gc.collect()
        except Exception as e:
            print(f"[PID {pid}] ‚ö† Erreur dans mesurer_temps_marche_pied_no (n={n}, seed={seed}): {e}", file=sys.stderr, flush=True)
            temps_marche_pied_no = 0.0
        
        try:
            c, s, d = clones()
            temps_marche_pied_bh = mesurer_temps_marche_pied_bh(c, s, d)
            del c, s, d
            gc.collect()
        except Exception as e:
            print(f"[PID {pid}] ‚ö† Erreur dans mesurer_temps_marche_pied_bh (n={n}, seed={seed}): {e}", file=sys.stderr, flush=True)
            temps_marche_pied_bh = 0.0
        
        print(f"[PID {pid}] ‚úì Termin√© (n={n}, seed={seed}): NO={temps_no:.6f}, BH={temps_bh:.6f}, MP_NO={temps_marche_pied_no:.6f}, MP_BH={temps_marche_pied_bh:.6f}", file=sys.stderr, flush=True)
        return temps_no, temps_bh, temps_marche_pied_no, temps_marche_pied_bh
    except Exception as e:
        # En cas d'erreur, retourner des valeurs par d√©faut pour √©viter de bloquer tout le processus
        print(f"[PID {pid}] ‚ö† Erreur dans l'ex√©cution (n={n}, seed={seed}): {e}", file=sys.stderr, flush=True)
        import traceback
        traceback.print_exc(file=sys.stderr)
        return 0.0, 0.0, 0.0, 0.0


def executer_etude_complexite(
    valeurs_n: List[int] = None,
    nb_executions: int = 100,
    sauvegarder_resultats: bool = True,
    utiliser_parallele: bool = False,  # D√âSACTIV√â PAR D√âFAUT pour respecter la contrainte "single processor"
    nb_processus: int = None,
    taille_lot: int = 10,
    pause_entre_lots: float = 0.1,
    fichier: str = 'complexite_resultats.json'
) -> Dict:
    # Alors l√†, cette fonction ex√©cute toute l'√©tude de complexit√©
    # En r√©sum√©, pour chaque valeur de n, on g√©n√®re 100 probl√®mes al√©atoires et on mesure les temps
    # Pour faire simple : on veut voir comment les algorithmes se comportent selon la taille
    # 
    # OPTIMISATION : Traitement par lots avec pauses pour √©viter la surchauffe du CPU
    
    # Pseudo-code :
    # SI valeurs_n n'est pas fourni:
    #     valeurs_n = [10, 40, 102, 400, 1000, 4000, 10000]  (valeurs demand√©es)
    # FIN SI
    # 
    # resultats = dictionnaire vide
    # 
    # POUR chaque n dans valeurs_n:
    #     theta_NO = []
    #     theta_BH = []
    #     t_NO = []
    #     t_BH = []
    #     
    #     POUR execution de 1 √† nb_executions:
    #         G√©n√©rer un probl√®me al√©atoire de taille n
    #         Mesurer theta_NO(n) et l'ajouter √† la liste
    #         Mesurer theta_BH(n) et l'ajouter √† la liste
    #         Mesurer t_NO(n) et l'ajouter √† la liste
    #         Mesurer t_BH(n) et l'ajouter √† la liste
    #     FIN POUR
    #     
    #     resultats[n] = {
    #         'theta_NO': theta_NO,
    #         'theta_BH': theta_BH,
    #         't_NO': t_NO,
    #         't_BH': t_BH,
    #         'theta_NO_plus_t_NO': [a+b for a,b in zip(theta_NO, t_NO)],
    #         'theta_BH_plus_t_BH': [a+b for a,b in zip(theta_BH, t_BH)]
    #     }
    # FIN POUR
    # 
    # SI sauvegarder_resultats:
    #     Sauvegarder les r√©sultats dans un fichier JSON
    # FIN SI
    # 
    # RETOURNER resultats
    
    if valeurs_n is None:
        valeurs_n = [10, 40, 102, 400, 1000, 4000, 10000]
    
    # OPTIMISATION : Pour n=10000, r√©duire le nombre d'ex√©cutions par d√©faut si non sp√©cifi√©
    # pour √©viter que le programme ne plante
    if 10000 in valeurs_n and nb_executions > 10:
        print(f"  ‚ö†Ô∏è  Pour n=10000, le nombre d'ex√©cutions est limit√© √† 10 pour √©viter les probl√®mes de m√©moire")
        print(f"  ‚ö†Ô∏è  Utilisez le param√®tre nb_executions pour modifier ce comportement")
        sys.stdout.flush()
        # Ne pas modifier nb_executions ici, mais avertir l'utilisateur
    
    # D√©terminer le nombre de processus √† utiliser (OPTIMISATION : laisser au moins 1-2 c≈ìurs libres)
    nb_processus = calculer_nb_processus_optimal(nb_processus)
    
    resultats = {}
    
    # Compteurs globaux pour l'estimation du temps
    total_executions_global = sum(nb_executions for _ in valeurs_n)
    execution_globale_actuelle = 0
    temps_debut_global = time.perf_counter()
    
    print(f"\n======================================================================")
    print(f"√âTUDE DE LA COMPLEXIT√â")
    print(f"======================================================================")
    print(f"Valeurs de n √† tester : {valeurs_n}")
    print(f"Nombre d'ex√©cutions par valeur de n : {nb_executions}")
    print(f"Total : {total_executions_global} ex√©cutions")
    print(f"Mode parall√®le : {'OUI' if utiliser_parallele else 'NON'} (utilisant {nb_processus if utiliser_parallele else 1}/{cpu_count()} processus)")
    if not utiliser_parallele:
        print(f"  ‚ÑπÔ∏è  Mode s√©quentiel (single processor) : conforme aux exigences du projet")
        print(f"  ‚ÑπÔ∏è  Garbage collection activ√© pour optimiser l'utilisation m√©moire (N=10000)")
    if utiliser_parallele:
        print(f"  üí° Optimisation : {max(0, cpu_count() - nb_processus)} c≈ìur(s) laiss√©(s) libre(s) pour √©viter la surchauffe")
        print(f"  üí° Traitement par lots de {taille_lot} avec pause de {pause_entre_lots}s entre les lots")
    print(f"\n‚ö† Attention : Cette op√©ration peut prendre beaucoup de temps !")
    print(f"======================================================================\n")
    sys.stdout.flush()
    
    for idx_n, n in enumerate(valeurs_n):
        print(f"\n>>> Traitement de n = {n} ({idx_n + 1}/{len(valeurs_n)}) <<<")
        print(f"G√©n√©ration de {nb_executions} probl√®me(s) al√©atoire(s)...")
        sys.stdout.flush()
        
        temps_debut_n = time.perf_counter()
        
        print(f"  üöÄ D√©marrage du traitement pour n={n}...")
        sys.stdout.flush()
        
        theta_NO = []
        theta_BH = []
        t_NO = []
        t_BH = []
        
        # Pour les petites valeurs de n, utiliser le mode s√©quentiel pour √©viter les blocages
        utiliser_parallele_effectif = utiliser_parallele
        nb_processus_effectif = nb_processus
        
        if n <= 10:
            # Pour n <= 10, utiliser le mode s√©quentiel pour √©viter les blocages
            utiliser_parallele_effectif = False
            print(f"  ‚ÑπÔ∏è  Mode s√©quentiel forc√© pour n={n} (petite taille)")
            sys.stdout.flush()
        elif n >= 1000:
            # Pour n >= 1000, utiliser le mode s√©quentiel pour √©viter la saturation m√©moire (N=10000 -> 800Mo par matrice !)
            # Sur Windows, multiprocessing 'spawn' copie tout, ce qui tue la RAM.
            utiliser_parallele_effectif = False
            print(f"  ‚ÑπÔ∏è  Mode s√©quentiel forc√© pour n={n} (grande taille) pour √©viter saturation m√©moire")
            sys.stdout.flush()
        elif n <= 100 and nb_executions <= 5:
            # Pour les petits probl√®mes, utiliser moins de processus
            nb_processus_effectif = min(4, nb_processus)
        elif n <= 400 and nb_executions <= 10:
            # Pour les probl√®mes moyens, utiliser un nombre mod√©r√©
            nb_processus_effectif = min(8, nb_processus)
        
        # G√©rer le cas o√π nb_executions = 1 avec parall√©lisation (utiliser quand m√™me le pool)
        if utiliser_parallele_effectif:
            # Version parall√©lis√©e avec traitement par lots pour √©viter la surchauffe
            # On cr√©e une liste de seeds pour chaque ex√©cution
            seeds = list(range(nb_executions))
            
            print(f"  üîÑ Initialisation du pool de {nb_processus_effectif} processus (sur {nb_processus} disponibles)...")
            sys.stdout.flush()
            
            # Utiliser multiprocessing pour parall√©liser les ex√©cutions
            with Pool(processes=nb_processus_effectif) as pool:
                # Cr√©er une fonction partielle avec n fix√©
                fonction_iteration = partial(executer_une_iteration_complete, n)
                
                # OPTIMISATION : Traitement par lots avec pauses pour √©viter la surchauffe
                resultats_iterations = []
                nb_lots = (nb_executions + taille_lot - 1) // taille_lot  # Arrondi sup√©rieur
                
                print(f"  üì¶ Traitement en {nb_lots} lot(s)...")
                sys.stdout.flush()
                
                for lot_num in range(nb_lots):
                    debut_lot = lot_num * taille_lot
                    fin_lot = min(debut_lot + taille_lot, nb_executions)
                    seeds_lot = seeds[debut_lot:fin_lot]
                    temps_debut_lot = time.perf_counter()
                    
                    print(f"  ‚öôÔ∏è  D√©marrage du lot {lot_num + 1}/{nb_lots} ({len(seeds_lot)} ex√©cution(s))...")
                    sys.stdout.flush()
                    
                    # Ex√©cuter le lot en parall√®le avec suivi de progression
                    # Utiliser map_async pour pouvoir surveiller la progression
                    resultat_async = pool.map_async(fonction_iteration, seeds_lot)
                    
                    # Calculer un timeout raisonnable bas√© sur n (plus n est grand, plus on attend)
                    # Estimation : pour n=10, ~0.1s par ex√©cution, pour n=10000, ~300s par ex√©cution
                    # Timeout plus court pour les petites valeurs de n
                    if n <= 10:
                        timeout_estime = 5 * len(seeds_lot)  # 5s par ex√©cution pour n=10
                    elif n <= 100:
                        timeout_estime = 30 * len(seeds_lot)  # 30s par ex√©cution pour n=100
                    else:
                        timeout_estime = max(60, n * n * 0.003 * len(seeds_lot))  # Timeout adaptatif
                    timeout_max = 3600  # Maximum 1 heure par lot
                    timeout_final = min(timeout_estime, timeout_max)
                    print(f"  ‚è± Timeout configur√© : {timeout_final:.0f}s pour n={n} ({len(seeds_lot)} ex√©cution(s))")
                    sys.stdout.flush()
                    
                    # Attendre avec heartbeat toutes les 30 secondes
                    dernier_heartbeat = time.perf_counter()
                    temps_attente = 0.0
                    timeout_atteint = False
                    resultats_lot = None
                    
                    while not resultat_async.ready():
                        time.sleep(0.5)  # V√©rifier toutes les 0.5 secondes
                        temps_actuel = time.perf_counter()
                        temps_attente = temps_actuel - temps_debut_lot
                        
                        # V√©rifier le timeout
                        if temps_attente > timeout_final:
                            print(f"  ‚ö† Timeout atteint pour le lot {lot_num + 1}/{nb_lots} (>{timeout_final:.0f}s)")
                            print(f"  ‚ö† Tentative d'annulation des t√¢ches...")
                            sys.stdout.flush()
                            
                            # Essayer d'annuler les t√¢ches si possible
                            try:
                                resultat_async.cancel()
                            except:
                                pass
                            
                            # Attendre un peu pour voir si les t√¢ches se terminent
                            time.sleep(2)
                            
                            if not resultat_async.ready():
                                print(f"  ‚ö† Les processus sont bloqu√©s, passage au lot suivant avec valeurs par d√©faut...")
                                sys.stdout.flush()
                                # Remplir avec des valeurs par d√©faut pour ne pas bloquer
                                resultats_lot = [(0.0, 0.0, 0.0, 0.0) for _ in seeds_lot]
                                timeout_atteint = True
                                print(f"  ‚ö† Lot {lot_num + 1} ignor√© √† cause du timeout")
                                sys.stdout.flush()
                                break  # Sortir de la boucle d'attente
                            else:
                                # Les t√¢ches se sont termin√©es entre-temps
                                print(f"  ‚úì Les t√¢ches se sont termin√©es apr√®s l'annonce du timeout")
                                sys.stdout.flush()
                                break
                        
                        # Heartbeat toutes les 30 secondes
                        if temps_actuel - dernier_heartbeat >= 30.0:
                            print(f"  üíì Programme actif... (lot {lot_num + 1}/{nb_lots} en cours depuis {temps_attente:.1f}s)")
                            sys.stdout.flush()
                            dernier_heartbeat = temps_actuel
                    
                    # R√©cup√©rer les r√©sultats avec un timeout raisonnable (seulement si on n'a pas d√©j√† eu un timeout)
                    if not timeout_atteint:
                        try:
                            # Timeout plus long pour permettre aux calculs de se terminer
                            resultats_lot = resultat_async.get(timeout=300)  # 5 minutes max pour r√©cup√©rer
                        except Exception as e:
                            print(f"  ‚ö† Erreur lors de la r√©cup√©ration des r√©sultats du lot {lot_num + 1}: {e}")
                            sys.stdout.flush()
                            # R√©essayer une fois avec un timeout plus long
                            try:
                                resultats_lot = resultat_async.get(timeout=600)  # 10 minutes
                            except Exception as e2:
                                print(f"  ‚úó √âchec d√©finitif pour le lot {lot_num + 1}: {e2}")
                                sys.stdout.flush()
                                # Remplir avec des valeurs par d√©faut pour ne pas bloquer
                                resultats_lot = [(0.0, 0.0, 0.0, 0.0) for _ in seeds_lot]
                    
                    resultats_iterations.extend(resultats_lot)
                    
                    temps_fin_lot = time.perf_counter()
                    temps_lot = temps_fin_lot - temps_debut_lot
                    
                    print(f"  ‚úì Lot {lot_num + 1}/{nb_lots} termin√© en {temps_lot:.2f}s")
                    sys.stdout.flush()
                    
                    # Mise √† jour de la progression globale
                    execution_globale_actuelle += len(seeds_lot)
                    executions_restantes_global = total_executions_global - execution_globale_actuelle
                    
                    # Calculer le temps √©coul√© et estimer le temps restant
                    temps_ecoule_total = time.perf_counter() - temps_debut_n
                    if fin_lot > 0:
                        temps_moyen_par_execution = temps_ecoule_total / fin_lot
                        executions_restantes_n = nb_executions - fin_lot
                        temps_restant_n = temps_moyen_par_execution * executions_restantes_n
                        
                        # Estimation pour les autres valeurs de n (approximative)
                        executions_restantes_autres_n = sum(
                            nb_executions for i, n_autre in enumerate(valeurs_n) 
                            if i > idx_n
                        )
                        temps_restant_autres_n = temps_moyen_par_execution * executions_restantes_autres_n * 1.2  # Facteur de s√©curit√©
                        temps_restant_total = temps_restant_n + temps_restant_autres_n
                        
                        heures_restantes = int(temps_restant_total // 3600)
                        minutes_restantes = int((temps_restant_total % 3600) // 60)
                        secondes_restantes = int(temps_restant_total % 60)
                        
                        # Afficher les logs d√©taill√©s (√† chaque lot)
                        progression_n = (fin_lot / nb_executions) * 100
                        progression_globale = (execution_globale_actuelle / total_executions_global) * 100
                        
                        print(f"\n  üìä Progression pour n={n}: {fin_lot}/{nb_executions} ({progression_n:.1f}%)")
                        print(f"  üìä Progression globale: {execution_globale_actuelle}/{total_executions_global} ({progression_globale:.1f}%)")
                        print(f"  ‚è± Temps √©coul√© pour n={n}: {temps_ecoule_total:.1f}s")
                        print(f"  ‚è≥ Temps restant estim√©: {heures_restantes}h {minutes_restantes}min {secondes_restantes}s")
                        print(f"  üîÑ Calculs restants: {executions_restantes_global} ex√©cutions")
                        print(f"  ‚ö° Dernier lot trait√© en {temps_lot:.2f}s ({len(seeds_lot)} ex√©cutions)")
                        print(f"  üì¶ Lot {lot_num + 1}/{nb_lots} termin√©")
                    
                    # OPTIMISATION : Garbage collection apr√®s chaque lot pour lib√©rer la m√©moire
                    # Particuli√®rement important pour N=10000 o√π les matrices sont tr√®s grandes
                    gc.collect()
                    
                    # Pause entre les lots pour permettre au CPU de se refroidir
                    if lot_num < nb_lots - 1:  # Pas de pause apr√®s le dernier lot
                        time.sleep(pause_entre_lots)
            
            # S√©parer les r√©sultats
            theta_NO = [r[0] for r in resultats_iterations]
            theta_BH = [r[1] for r in resultats_iterations]
            t_NO = [r[2] for r in resultats_iterations]
            t_BH = [r[3] for r in resultats_iterations]
            
            # OPTIMISATION : Garbage collection apr√®s avoir trait√© tous les lots
            gc.collect()
        else:
            # Version s√©quentielle (pour comparaison ou si parall√©lisation d√©sactiv√©e)
            print(f"  üîÑ Mode s√©quentiel activ√©...")
            sys.stdout.flush()
            
            theta_NO = []
            theta_BH = []
            t_NO = []
            t_BH = []
            
            dernier_heartbeat = time.perf_counter()
            
            for execution in range(nb_executions):
                temps_debut_exec = time.perf_counter()
                
                print(f"  ‚öôÔ∏è  Ex√©cution {execution + 1}/{nb_executions} en cours...")
                sys.stdout.flush()
                
                # G√©n√©rer un probl√®me al√©atoire (on utilise execution comme seed pour reproductibilit√©)
                costs, supplies, demands = generer_probleme_aleatoire(n, seed=execution)
                
                # Mesurer theta_NO(n)
                print(f"    ‚Üí Mesure Œ∏NO(n)...")
                sys.stdout.flush()
                temps_no = mesurer_temps_nord_ouest(costs, supplies, demands)
                theta_NO.append(temps_no)
                
                # Mesurer theta_BH(n)
                print(f"    ‚Üí Mesure Œ∏BH(n)...")
                sys.stdout.flush()
                temps_bh = mesurer_temps_balas_hammer(costs, supplies, demands)
                theta_BH.append(temps_bh)
                
                # Mesurer t_NO(n)
                print(f"    ‚Üí Mesure tNO(n) (marche-pied avec NO)...")
                sys.stdout.flush()
                temps_marche_pied_no = mesurer_temps_marche_pied_no(costs, supplies, demands)
                t_NO.append(temps_marche_pied_no)
                
                # Mesurer t_BH(n)
                print(f"    ‚Üí Mesure tBH(n) (marche-pied avec BH)...")
                sys.stdout.flush()
                temps_marche_pied_bh = mesurer_temps_marche_pied_bh(costs, supplies, demands)
                t_BH.append(temps_marche_pied_bh)
                
                temps_fin_exec = time.perf_counter()
                temps_exec = temps_fin_exec - temps_debut_exec
                
                # OPTIMISATION : Garbage collection apr√®s chaque ex√©cution pour lib√©rer la m√©moire
                # Particuli√®rement important pour N=10000 o√π les matrices sont tr√®s grandes
                gc.collect()
                
                # Heartbeat toutes les 30 secondes
                temps_actuel = time.perf_counter()
                if temps_actuel - dernier_heartbeat >= 30.0:
                    print(f"  üíì Programme actif... (ex√©cution {execution + 1}/{nb_executions}, {temps_ecoule_total:.1f}s √©coul√©es)")
                    sys.stdout.flush()
                    dernier_heartbeat = temps_actuel
        
        temps_fin_n = time.perf_counter()
        temps_total_n = temps_fin_n - temps_debut_n
        
        # Calculer les moyennes
        moyenne_theta_NO = sum(theta_NO) / len(theta_NO) if theta_NO else 0
        moyenne_theta_BH = sum(theta_BH) / len(theta_BH) if theta_BH else 0
        moyenne_t_NO = sum(t_NO) / len(t_NO) if t_NO else 0
        moyenne_t_BH = sum(t_BH) / len(t_BH) if t_BH else 0
        
        resultats[n] = {
            'theta_NO': theta_NO,
            'theta_BH': theta_BH,
            't_NO': t_NO,
            't_BH': t_BH,
            'theta_NO_plus_t_NO': [a+b for a,b in zip(theta_NO, t_NO)],
            'theta_BH_plus_t_BH': [a+b for a,b in zip(theta_BH, t_BH)]
        }
        
        print(f"\n  ‚úì Termin√© pour n={n} ({nb_executions} ex√©cutions)")
        print(f"  ‚è± Temps total: {int(temps_total_n // 3600)}h {int((temps_total_n % 3600) // 60)}min {int(temps_total_n % 60)}s ({temps_total_n:.2f}s)")
        
        # Estimation du temps restant global
        if idx_n < len(valeurs_n) - 1:
            temps_ecoule_global = time.perf_counter() - temps_debut_global
            pourcentage_fait = (execution_globale_actuelle / total_executions_global)
            if pourcentage_fait > 0:
                temps_total_estime = temps_ecoule_global / pourcentage_fait
                temps_restant_estime = temps_total_estime - temps_ecoule_global
                h_rest = int(temps_restant_estime // 3600)
                m_rest = int((temps_restant_estime % 3600) // 60)
                s_rest = int(temps_restant_estime % 60)
                print(f"  ‚è≥ Temps restant estim√© (toutes valeurs): {h_rest}h {m_rest}min {s_rest}s")
        
        print(f"  Temps moyen Œ∏NO(n) : {moyenne_theta_NO:.6f} s")
        print(f"  Temps moyen Œ∏BH(n) : {moyenne_theta_BH:.6f} s")
        print(f"  Temps moyen tNO(n) : {moyenne_t_NO:.6f} s")
        print(f"  Temps moyen tBH(n) : {moyenne_t_BH:.6f} s")
        sys.stdout.flush()
        
        # Sauvegarder les r√©sultats interm√©diaires (on ne sait jamais, si √ßa plante)
        if sauvegarder_resultats:
            # Cr√©er le dossier si n√©cessaire
            dossier = os.path.dirname(fichier)
            if dossier and not os.path.exists(dossier):
                os.makedirs(dossier, exist_ok=True)
            charger_resultats_complexite(resultats, fichier)
        
        # OPTIMISATION : Garbage collection apr√®s chaque valeur de n pour lib√©rer la m√©moire
        # Particuli√®rement important pour N=10000 o√π les matrices sont tr√®s grandes
        gc.collect()
    
    if sauvegarder_resultats:
        # Cr√©er le dossier si n√©cessaire
        dossier = os.path.dirname(fichier)
        if dossier and not os.path.exists(dossier):
            os.makedirs(dossier, exist_ok=True)
        charger_resultats_complexite(resultats, fichier)
        print(f"\n‚úì R√©sultats sauvegard√©s dans '{fichier}'")
    
    # Dernier garbage collection avant de retourner
    gc.collect()
    
    return resultats

def charger_resultats_complexite(nouveaux_resultats: Dict = None, fichier: str = 'complexite_resultats.json') -> Dict:
    """
    Charge les r√©sultats existants et les met √† jour avec les nouveaux r√©sultats.
    """
    resultats = {}
    if os.path.exists(fichier):
        try:
            with open(fichier, 'r') as f:
                resultats = json.load(f)
        except json.JSONDecodeError:
            pass
    
    if nouveaux_resultats:
        # Mettre √† jour avec les nouveaux r√©sultats
        resultats.update(nouveaux_resultats)
        
        # Sauvegarder
        with open(fichier, 'w') as f:
            json.dump(resultats, f, indent=4)
        
    return resultats

def tracer_nuages_de_points(resultats: Dict):
    """
    Trace les nuages de points des temps d'ex√©cution en fonction de n.
    """
    if not MATPLOTLIB_AVAILABLE:
        print("‚ö† Matplotlib n'est pas install√©. Impossible de tracer les graphiques.")
        return

    # Pr√©paration des donn√©es
    valeurs_n = sorted([int(k) for k in resultats.keys()])
    
    plt.figure(figsize=(15, 10))
    
    # 1. Nord-Ouest vs Balas-Hammer (Initial)
    plt.subplot(2, 2, 1)
    for n in valeurs_n:
        data = resultats[str(n)]
        plt.scatter([n]*len(data['theta_NO']), data['theta_NO'], c='blue', alpha=0.5, s=10)
        plt.scatter([n]*len(data['theta_BH']), data['theta_BH'], c='red', alpha=0.5, s=10)
    plt.plot(valeurs_n, [sum(resultats[str(n)]['theta_NO'])/len(resultats[str(n)]['theta_NO']) for n in valeurs_n], 'b-', label='Nord-Ouest')
    plt.plot(valeurs_n, [sum(resultats[str(n)]['theta_BH'])/len(resultats[str(n)]['theta_BH']) for n in valeurs_n], 'r-', label='Balas-Hammer')
    plt.xlabel('Taille n')
    plt.ylabel('Temps (s)')
    plt.title('Comparaison Algorithmes Initiaux')
    plt.legend()
    plt.grid(True)
    
    # 2. Marche-Pied (NO) vs Marche-Pied (BH)
    plt.subplot(2, 2, 2)
    for n in valeurs_n:
        data = resultats[str(n)]
        plt.scatter([n]*len(data['t_NO']), data['t_NO'], c='green', alpha=0.5, s=10)
        plt.scatter([n]*len(data['t_BH']), data['t_BH'], c='orange', alpha=0.5, s=10)
    plt.plot(valeurs_n, [sum(resultats[str(n)]['t_NO'])/len(resultats[str(n)]['t_NO']) for n in valeurs_n], 'g-', label='Marche-Pied (NO)')
    plt.plot(valeurs_n, [sum(resultats[str(n)]['t_BH'])/len(resultats[str(n)]['t_BH']) for n in valeurs_n], color='orange', linestyle='-', label='Marche-Pied (BH)')
    plt.xlabel('Taille n')
    plt.ylabel('Temps (s)')
    plt.title('Comparaison Optimisation Marche-Pied')
    plt.legend()
    plt.grid(True)
    
    # 3. Total (Initial + Optimisation)
    plt.subplot(2, 2, 3)
    for n in valeurs_n:
        data = resultats[str(n)]
        total_no = data['theta_NO_plus_t_NO']
        total_bh = data['theta_BH_plus_t_BH']
        plt.scatter([n]*len(total_no), total_no, c='purple', alpha=0.5, s=10)
        plt.scatter([n]*len(total_bh), total_bh, c='brown', alpha=0.5, s=10)
    plt.plot(valeurs_n, [sum(resultats[str(n)]['theta_NO_plus_t_NO'])/len(resultats[str(n)]['theta_NO_plus_t_NO']) for n in valeurs_n], color='purple', linestyle='-', label='Total (NO)')
    plt.plot(valeurs_n, [sum(resultats[str(n)]['theta_BH_plus_t_BH'])/len(resultats[str(n)]['theta_BH_plus_t_BH']) for n in valeurs_n], color='brown', linestyle='-', label='Total (BH)')
    plt.xlabel('Taille n')
    plt.ylabel('Temps (s)')
    plt.title('Temps Total de R√©solution')
    plt.legend()
    plt.grid(True)
    
    plt.tight_layout()
    plt.show()

def determiner_complexite_pire_cas(resultats: Dict):
    """
    Analyse la complexit√© dans le pire des cas en tra√ßant les courbes max.
    """
    if not MATPLOTLIB_AVAILABLE:
        print("‚ö† Matplotlib n'est pas install√©. Impossible de tracer les graphiques.")
        return

    valeurs_n = sorted([int(k) for k in resultats.keys()])
    
    # R√©cup√©rer les maximums
    max_no = [max(resultats[str(n)]['theta_NO']) for n in valeurs_n]
    max_bh = [max(resultats[str(n)]['theta_BH']) for n in valeurs_n]
    max_mp_no = [max(resultats[str(n)]['t_NO']) for n in valeurs_n]
    
    plt.figure(figsize=(15, 5))
    
    # Analyse NO (th√©orique O(nm) -> O(n^2))
    plt.subplot(1, 3, 1)
    plt.plot(valeurs_n, max_no, 'bo-', label='Max NO')
    # Courbe th√©orique ajust√©e (sommaire)
    scale = max_no[-1] / (valeurs_n[-1]**2)
    plt.plot(valeurs_n, [scale * n**2 for n in valeurs_n], 'k--', label=f'O(n^2)')
    plt.title('Complexit√© Pire Cas : Nord-Ouest')
    plt.legend()
    plt.grid(True)
    
    # Analyse BH (th√©orique plus complexe, souvent O(n^3) ou O(n^4))
    plt.subplot(1, 3, 2)
    plt.plot(valeurs_n, max_bh, 'ro-', label='Max BH')
    scale = max_bh[-1] / (valeurs_n[-1]**3)
    plt.plot(valeurs_n, [scale * n**3 for n in valeurs_n], 'k--', label=f'O(n^3)')
    plt.title('Complexit√© Pire Cas : Balas-Hammer')
    plt.legend()
    plt.grid(True)
    
    # Analyse Marche-Pied
    plt.subplot(1, 3, 3)
    plt.plot(valeurs_n, max_mp_no, 'go-', label='Max Marche-Pied')
    plt.title('Complexit√© Pire Cas : Marche-Pied')
    plt.legend()
    plt.grid(True)
    
    plt.tight_layout()
    plt.show()

def comparer_algorithmes(resultats: Dict):
    """
    Affiche des graphiques comparatifs (bar charts) des temps moyens.
    """
    if not MATPLOTLIB_AVAILABLE:
        print("‚ö† Matplotlib n'est pas install√©. Impossible de tracer les graphiques.")
        return

    valeurs_n = sorted([int(k) for k in resultats.keys()])
    
    avg_no = [sum(resultats[str(n)]['theta_NO'])/len(resultats[str(n)]['theta_NO']) for n in valeurs_n]
    avg_bh = [sum(resultats[str(n)]['theta_BH'])/len(resultats[str(n)]['theta_BH']) for n in valeurs_n]
    avg_total_no = [sum(resultats[str(n)]['theta_NO_plus_t_NO'])/len(resultats[str(n)]['theta_NO_plus_t_NO']) for n in valeurs_n]
    avg_total_bh = [sum(resultats[str(n)]['theta_BH_plus_t_BH'])/len(resultats[str(n)]['theta_BH_plus_t_BH']) for n in valeurs_n]
    
    plt.figure(figsize=(12, 6))
    
    # Comparaison Temps Initial
    plt.subplot(1, 2, 1)
    x = range(len(valeurs_n))
    width = 0.35
    plt.bar([i - width/2 for i in x], avg_no, width, label='Nord-Ouest', color='blue')
    plt.bar([i + width/2 for i in x], avg_bh, width, label='Balas-Hammer', color='red')
    plt.xticks(x, valeurs_n)
    plt.xlabel('Taille n')
    plt.ylabel('Temps moyen (s)')
    plt.title('Comparaison Solutions Initiales')
    plt.legend()
    
    # Comparaison Temps Total
    plt.subplot(1, 2, 2)
    plt.bar([i - width/2 for i in x], avg_total_no, width, label='Total (NO)', color='purple')
    plt.bar([i + width/2 for i in x], avg_total_bh, width, label='Total (BH)', color='brown')
    plt.xticks(x, valeurs_n)
    plt.xlabel('Taille n')
    plt.ylabel('Temps moyen (s)')
    plt.title('Comparaison Temps Total de R√©solution')
    plt.legend()
    
    plt.tight_layout()
    plt.show()
